{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#data = pickle.load( open( \"./template/trainingdata.p\", \"rb\" ), encoding=\"latin1\" )\n",
    "#data = pickle.load(open(\"trainingdata_all.pickle\", \"rb\"), encoding=\"latin1\")\n",
    "data = pickle.load(open(\"label_resize.pickle\", \"rb\"))\n",
    "n_images = len(data)\n",
    "#test, training = data[0:int(n_images/3)], data[int(n_images/3):]\n",
    "training, test = train_test_split(data, test_size=0.33, random_state=321)\n",
    "\n",
    "def get_training_data():\n",
    "\n",
    "    trX = np.array([np.reshape(a[2],a[2].shape[0]*a[2].shape[1]) for a in training])\n",
    "    print(np.shape(trX)[1])\n",
    "    trY = np.zeros((len(training)),dtype=np.float)\n",
    "    for i, data in enumerate(training):\n",
    "        trY[i] = float(data[0])\n",
    "    return trX, trY\n",
    "\n",
    "def get_test_data():\n",
    "    teX = np.array([np.reshape(a[2],a[2].shape[0]*a[2].shape[1]) for a in test])\n",
    "    teY = np.zeros((len(test)),dtype=np.float)\n",
    "    for i, data in enumerate(test):\n",
    "        teY[i] = float(data[0])\n",
    "    return teX,teY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1106"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import tensorflow as tf\n",
    "#import pickle\n",
    "#from get_image_data import *\n",
    "\n",
    "class DNN_Driver():\n",
    "    def __init__(self):\n",
    "        self.trX = None\n",
    "        self.trY = None\n",
    "        self.teX = None\n",
    "        self.teY = None\n",
    "        self.model = None\n",
    "\n",
    "    def tf_learn(self):\n",
    "        self.trX, self.trY = get_training_data()\n",
    "        self.teX, self.teY = get_test_data()\n",
    "\n",
    "        seed = 0\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "        self.model=Sequential()\n",
    "        self.model.add(Dense(512, input_dim=np.shape(self.trX)[1], activation='relu'))\n",
    "        self.model.add(Dense(64, activation='relu'))\n",
    "        self.model.add(Dense(1))\n",
    "\n",
    "        self.model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "        self.model.fit(self.trX, self.trY, epochs=6, batch_size=1)\n",
    "        \n",
    "        Y_prediction = self.model.predict(self.teX).flatten()\n",
    "\n",
    "        for i in range(len(self.teY)):\n",
    "            label = self.teY[i]\n",
    "            pred = Y_prediction[i]\n",
    "            print(\"label:{:.2f}, pred:{:.2f}\".format(label, pred))\n",
    "        return\n",
    "\n",
    "    \n",
    "    def get_direction(img):\n",
    "        print(img.shape)\n",
    "    #    img = np.array([np.reshape(img,img.shape**2)])\n",
    "        ret =  self.model.predict(np.array([img]))\n",
    "        return ret\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    def predict_direction(self, img):\n",
    "        print(img.shape)\n",
    "#        img = np.array([np.reshape(img,img.shape**2)])\n",
    "        ret =  self.model.predict(np.array([img]))\n",
    "        return ret\n",
    "\n",
    "    def get_test_img(self):\n",
    "        img = self.teX[10]\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741 365\n"
     ]
    }
   ],
   "source": [
    "print(len(training), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072\n",
      "Epoch 1/6\n",
      "741/741 [==============================] - 7s 10ms/step - loss: 9428.1123\n",
      "Epoch 2/6\n",
      "741/741 [==============================] - 7s 10ms/step - loss: 368.8466\n",
      "Epoch 3/6\n",
      "741/741 [==============================] - 7s 9ms/step - loss: 343.1923\n",
      "Epoch 4/6\n",
      "741/741 [==============================] - 7s 10ms/step - loss: 39.2189\n",
      "Epoch 5/6\n",
      "741/741 [==============================] - 7s 9ms/step - loss: 29.2314\n",
      "Epoch 6/6\n",
      "741/741 [==============================] - 7s 9ms/step - loss: 25.1900\n",
      "label:0.00, pred:1.79\n",
      "label:0.00, pred:-2.17\n",
      "label:0.00, pred:-1.61\n",
      "label:-0.50, pred:2.08\n",
      "label:0.00, pred:3.52\n",
      "label:-0.50, pred:1.10\n",
      "label:0.00, pred:0.99\n",
      "label:0.00, pred:3.53\n",
      "label:0.00, pred:-0.10\n",
      "label:0.00, pred:1.51\n",
      "label:0.00, pred:3.24\n",
      "label:-0.50, pred:1.57\n",
      "label:0.50, pred:1.03\n",
      "label:-1.00, pred:4.80\n",
      "label:0.00, pred:1.68\n",
      "label:-0.50, pred:1.40\n",
      "label:1.00, pred:3.84\n",
      "label:0.00, pred:-0.05\n",
      "label:0.00, pred:-1.36\n",
      "label:0.00, pred:1.28\n",
      "label:0.00, pred:0.48\n",
      "label:0.00, pred:0.53\n",
      "label:0.50, pred:-0.14\n",
      "label:-0.50, pred:-1.96\n",
      "label:0.00, pred:-0.82\n",
      "label:0.00, pred:-0.36\n",
      "label:-0.50, pred:1.06\n",
      "label:0.50, pred:-4.30\n",
      "label:0.00, pred:-2.45\n",
      "label:0.00, pred:2.91\n",
      "label:0.00, pred:3.89\n",
      "label:0.00, pred:-1.80\n",
      "label:0.50, pred:3.17\n",
      "label:-0.50, pred:0.47\n",
      "label:0.00, pred:-0.35\n",
      "label:0.00, pred:-1.61\n",
      "label:0.00, pred:-1.34\n",
      "label:0.00, pred:1.23\n",
      "label:0.00, pred:0.96\n",
      "label:-0.50, pred:1.05\n",
      "label:-0.50, pred:0.35\n",
      "label:0.00, pred:-1.17\n",
      "label:-1.00, pred:1.52\n",
      "label:-0.50, pred:1.46\n",
      "label:0.50, pred:1.55\n",
      "label:0.00, pred:-2.73\n",
      "label:0.50, pred:-1.52\n",
      "label:0.00, pred:1.56\n",
      "label:0.50, pred:2.87\n",
      "label:0.00, pred:1.04\n",
      "label:0.00, pred:0.10\n",
      "label:0.50, pred:-1.02\n",
      "label:-0.50, pred:-2.93\n",
      "label:0.00, pred:-0.78\n",
      "label:0.00, pred:1.58\n",
      "label:0.00, pred:-2.23\n",
      "label:0.00, pred:-1.73\n",
      "label:-1.00, pred:0.39\n",
      "label:0.00, pred:0.11\n",
      "label:0.00, pred:0.90\n",
      "label:0.00, pred:1.76\n",
      "label:0.00, pred:3.58\n",
      "label:0.00, pred:-2.26\n",
      "label:0.50, pred:4.77\n",
      "label:-0.50, pred:3.22\n",
      "label:0.00, pred:1.40\n",
      "label:-0.50, pred:-1.26\n",
      "label:0.00, pred:-3.73\n",
      "label:0.00, pred:-0.64\n",
      "label:-0.50, pred:2.13\n",
      "label:0.00, pred:-0.21\n",
      "label:0.00, pred:0.66\n",
      "label:0.00, pred:-0.57\n",
      "label:0.00, pred:-0.54\n",
      "label:0.00, pred:-2.40\n",
      "label:-0.50, pred:1.37\n",
      "label:0.00, pred:-0.08\n",
      "label:-0.50, pred:-0.32\n",
      "label:0.00, pred:1.50\n",
      "label:-0.50, pred:1.33\n",
      "label:-0.50, pred:-1.49\n",
      "label:0.00, pred:0.52\n",
      "label:-0.50, pred:-0.68\n",
      "label:0.00, pred:0.94\n",
      "label:-0.50, pred:1.05\n",
      "label:0.50, pred:2.63\n",
      "label:0.00, pred:3.87\n",
      "label:0.00, pred:1.69\n",
      "label:-0.50, pred:0.72\n",
      "label:0.00, pred:2.71\n",
      "label:0.50, pred:2.48\n",
      "label:0.00, pred:-1.84\n",
      "label:0.00, pred:-1.70\n",
      "label:0.50, pred:-1.56\n",
      "label:0.00, pred:-2.53\n",
      "label:0.00, pred:-0.20\n",
      "label:0.50, pred:3.78\n",
      "label:0.00, pred:2.87\n",
      "label:0.50, pred:3.35\n",
      "label:-0.50, pred:-1.09\n",
      "label:0.50, pred:0.43\n",
      "label:0.00, pred:1.23\n",
      "label:0.50, pred:0.09\n",
      "label:0.50, pred:3.16\n",
      "label:0.00, pred:1.35\n",
      "label:0.00, pred:-1.78\n",
      "label:0.50, pred:1.76\n",
      "label:0.00, pred:-1.78\n",
      "label:0.00, pred:1.51\n",
      "label:-1.00, pred:1.50\n",
      "label:0.00, pred:0.68\n",
      "label:0.00, pred:2.39\n",
      "label:0.00, pred:-2.07\n",
      "label:0.00, pred:-0.06\n",
      "label:0.00, pred:-1.23\n",
      "label:-0.50, pred:0.48\n",
      "label:0.00, pred:0.91\n",
      "label:0.00, pred:1.83\n",
      "label:0.50, pred:3.25\n",
      "label:0.00, pred:0.31\n",
      "label:-0.50, pred:1.68\n",
      "label:0.00, pred:-0.68\n",
      "label:-1.00, pred:-0.12\n",
      "label:0.00, pred:0.66\n",
      "label:0.00, pred:0.74\n",
      "label:0.00, pred:3.71\n",
      "label:0.00, pred:-0.66\n",
      "label:0.00, pred:-1.35\n",
      "label:-0.50, pred:0.15\n",
      "label:0.00, pred:-0.37\n",
      "label:0.00, pred:1.54\n",
      "label:-0.50, pred:0.84\n",
      "label:0.00, pred:-2.74\n",
      "label:-0.50, pred:-2.59\n",
      "label:-0.50, pred:0.18\n",
      "label:0.00, pred:-2.29\n",
      "label:0.50, pred:-1.50\n",
      "label:0.00, pred:3.72\n",
      "label:0.00, pred:-0.35\n",
      "label:0.00, pred:1.72\n",
      "label:0.50, pred:-0.14\n",
      "label:0.00, pred:-1.02\n",
      "label:-0.50, pred:-0.36\n",
      "label:0.00, pred:1.32\n",
      "label:0.00, pred:0.96\n",
      "label:0.00, pred:0.61\n",
      "label:0.50, pred:0.92\n",
      "label:0.00, pred:2.54\n",
      "label:0.00, pred:-2.52\n",
      "label:-0.50, pred:3.20\n",
      "label:0.00, pred:-2.91\n",
      "label:0.00, pred:1.02\n",
      "label:0.00, pred:1.85\n",
      "label:0.00, pred:4.24\n",
      "label:0.50, pred:1.69\n",
      "label:1.00, pred:0.41\n",
      "label:0.00, pred:2.30\n",
      "label:0.50, pred:-1.35\n",
      "label:0.00, pred:1.08\n",
      "label:0.00, pred:1.36\n",
      "label:0.50, pred:2.27\n",
      "label:-0.50, pred:0.58\n",
      "label:0.00, pred:-1.30\n",
      "label:0.00, pred:-0.98\n",
      "label:0.00, pred:-0.36\n",
      "label:0.00, pred:-0.23\n",
      "label:0.50, pred:-0.46\n",
      "label:-0.50, pred:1.53\n",
      "label:0.00, pred:0.88\n",
      "label:0.00, pred:1.42\n",
      "label:1.00, pred:0.59\n",
      "label:0.00, pred:-3.31\n",
      "label:0.00, pred:4.05\n",
      "label:0.00, pred:-1.76\n",
      "label:-0.50, pred:1.01\n",
      "label:0.00, pred:2.86\n",
      "label:0.00, pred:-3.38\n",
      "label:0.00, pred:0.59\n",
      "label:-0.50, pred:1.21\n",
      "label:-0.50, pred:1.82\n",
      "label:0.00, pred:4.09\n",
      "label:0.00, pred:1.94\n",
      "label:0.00, pred:2.30\n",
      "label:-0.50, pred:0.83\n",
      "label:-0.50, pred:2.28\n",
      "label:0.00, pred:1.29\n",
      "label:0.00, pred:0.27\n",
      "label:0.00, pred:2.30\n",
      "label:0.00, pred:1.87\n",
      "label:0.00, pred:1.10\n",
      "label:0.50, pred:3.40\n",
      "label:0.50, pred:2.61\n",
      "label:-0.50, pred:0.96\n",
      "label:0.00, pred:-0.58\n",
      "label:-0.50, pred:-0.66\n",
      "label:0.00, pred:3.33\n",
      "label:-0.50, pred:1.27\n",
      "label:0.00, pred:-2.52\n",
      "label:-0.50, pred:-1.34\n",
      "label:0.00, pred:1.23\n",
      "label:0.00, pred:0.19\n",
      "label:0.00, pred:-0.19\n",
      "label:0.00, pred:1.52\n",
      "label:0.00, pred:2.71\n",
      "label:0.50, pred:1.52\n",
      "label:0.00, pred:0.34\n",
      "label:-0.50, pred:-2.97\n",
      "label:-0.50, pred:-2.90\n",
      "label:-0.50, pred:0.26\n",
      "label:0.00, pred:-1.43\n",
      "label:0.00, pred:2.45\n",
      "label:0.50, pred:3.68\n",
      "label:0.50, pred:0.74\n",
      "label:0.00, pred:-0.21\n",
      "label:0.00, pred:3.38\n",
      "label:0.50, pred:-0.32\n",
      "label:0.00, pred:1.59\n",
      "label:-0.50, pred:0.68\n",
      "label:0.00, pred:2.16\n",
      "label:0.00, pred:0.00\n",
      "label:-0.50, pred:1.09\n",
      "label:0.00, pred:0.52\n",
      "label:0.00, pred:1.66\n",
      "label:0.00, pred:0.16\n",
      "label:0.00, pred:2.03\n",
      "label:0.00, pred:2.17\n",
      "label:-0.50, pred:-0.70\n",
      "label:0.00, pred:3.06\n",
      "label:0.00, pred:1.92\n",
      "label:0.00, pred:0.21\n",
      "label:0.00, pred:2.96\n",
      "label:-0.50, pred:1.53\n",
      "label:0.00, pred:-1.27\n",
      "label:0.00, pred:-2.25\n",
      "label:0.00, pred:0.06\n",
      "label:0.50, pred:0.15\n",
      "label:0.00, pred:-0.11\n",
      "label:0.00, pred:2.90\n",
      "label:0.00, pred:0.30\n",
      "label:-0.50, pred:0.55\n",
      "label:0.00, pred:1.10\n",
      "label:0.00, pred:0.35\n",
      "label:0.00, pred:-0.82\n",
      "label:0.00, pred:-0.10\n",
      "label:-0.50, pred:0.11\n",
      "label:0.00, pred:0.88\n",
      "label:-0.50, pred:0.85\n",
      "label:0.00, pred:5.13\n",
      "label:-0.50, pred:2.02\n",
      "label:0.00, pred:-1.44\n",
      "label:0.00, pred:1.62\n",
      "label:0.00, pred:-1.72\n",
      "label:-0.50, pred:0.52\n",
      "label:-0.50, pred:3.94\n",
      "label:0.00, pred:2.15\n",
      "label:0.00, pred:0.50\n",
      "label:0.00, pred:2.58\n",
      "label:0.00, pred:-0.74\n",
      "label:-0.50, pred:0.56\n",
      "label:0.00, pred:-1.32\n",
      "label:0.50, pred:5.18\n",
      "label:0.00, pred:1.60\n",
      "label:0.00, pred:2.00\n",
      "label:0.00, pred:-1.34\n",
      "label:0.00, pred:2.98\n",
      "label:0.00, pred:2.48\n",
      "label:0.50, pred:-0.61\n",
      "label:-0.50, pred:2.00\n",
      "label:-0.50, pred:1.94\n",
      "label:0.00, pred:4.24\n",
      "label:-1.00, pred:-2.17\n",
      "label:0.00, pred:0.55\n",
      "label:0.50, pred:4.77\n",
      "label:-0.50, pred:0.12\n",
      "label:0.00, pred:-3.63\n",
      "label:0.00, pred:-2.56\n",
      "label:0.00, pred:-1.43\n",
      "label:0.50, pred:2.75\n",
      "label:0.00, pred:3.38\n",
      "label:0.00, pred:0.31\n",
      "label:-0.50, pred:1.29\n",
      "label:-0.50, pred:0.00\n",
      "label:0.00, pred:4.73\n",
      "label:-0.50, pred:-2.93\n",
      "label:0.00, pred:0.24\n",
      "label:-0.50, pred:3.21\n",
      "label:0.00, pred:2.31\n",
      "label:0.00, pred:1.91\n",
      "label:-0.50, pred:3.34\n",
      "label:0.50, pred:4.09\n",
      "label:0.00, pred:1.75\n",
      "label:-0.50, pred:1.59\n",
      "label:0.00, pred:-2.32\n",
      "label:0.50, pred:6.90\n",
      "label:-0.50, pred:1.41\n",
      "label:0.00, pred:2.71\n",
      "label:0.00, pred:3.51\n",
      "label:0.50, pred:-2.20\n",
      "label:0.00, pred:0.29\n",
      "label:-0.50, pred:0.63\n",
      "label:0.00, pred:-1.36\n",
      "label:-0.50, pred:1.78\n",
      "label:-0.50, pred:0.53\n",
      "label:0.00, pred:-0.86\n",
      "label:-1.00, pred:-1.45\n",
      "label:0.00, pred:-2.36\n",
      "label:0.00, pred:-0.11\n",
      "label:0.00, pred:-2.15\n",
      "label:0.50, pred:-0.91\n",
      "label:0.00, pred:-0.42\n",
      "label:-0.50, pred:0.60\n",
      "label:0.00, pred:-0.47\n",
      "label:0.00, pred:-0.95\n",
      "label:0.50, pred:2.30\n",
      "label:0.00, pred:2.47\n",
      "label:0.00, pred:1.38\n",
      "label:0.00, pred:-3.65\n",
      "label:0.00, pred:2.58\n",
      "label:0.50, pred:2.38\n",
      "label:0.00, pred:-0.81\n",
      "label:0.00, pred:-0.56\n",
      "label:0.00, pred:-0.62\n",
      "label:-0.50, pred:1.15\n",
      "label:0.00, pred:-1.03\n",
      "label:0.00, pred:-0.22\n",
      "label:0.00, pred:0.55\n",
      "label:0.00, pred:1.73\n",
      "label:0.00, pred:2.45\n",
      "label:0.00, pred:-0.80\n",
      "label:0.00, pred:-0.55\n",
      "label:0.00, pred:1.68\n",
      "label:0.50, pred:0.32\n",
      "label:0.00, pred:4.15\n",
      "label:0.00, pred:2.39\n",
      "label:0.00, pred:-1.54\n",
      "label:0.50, pred:2.68\n",
      "label:0.00, pred:2.23\n",
      "label:-0.50, pred:1.59\n",
      "label:1.00, pred:2.84\n",
      "label:0.00, pred:-0.28\n",
      "label:-0.50, pred:0.58\n",
      "label:-0.50, pred:-1.03\n",
      "label:-1.00, pred:-3.71\n",
      "label:0.00, pred:1.94\n",
      "label:0.00, pred:2.87\n",
      "label:-0.50, pred:1.79\n",
      "label:0.00, pred:3.88\n",
      "label:0.50, pred:4.00\n",
      "label:0.00, pred:-0.81\n",
      "label:0.00, pred:0.62\n",
      "label:0.00, pred:-1.54\n",
      "label:0.00, pred:2.39\n",
      "label:0.00, pred:0.82\n",
      "label:0.00, pred:0.99\n",
      "label:-0.50, pred:0.53\n",
      "label:0.00, pred:-0.77\n",
      "label:-0.50, pred:0.04\n",
      "label:0.00, pred:0.42\n",
      "label:0.50, pred:3.69\n",
      "label:-0.50, pred:1.46\n",
      "label:0.00, pred:-0.36\n",
      "label:0.00, pred:2.83\n",
      "label:-0.50, pred:0.76\n",
      "label:0.00, pred:0.11\n",
      "label:0.00, pred:-0.16\n"
     ]
    }
   ],
   "source": [
    "dnn_driver = DNN_Driver()\n",
    "dnn_driver.tf_learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기록 (add2)\n",
    "- trainingdata_add2.pickle, test_size = 0.33, random_state = 123, epoch 6\n",
    "    - Epoch 1/6\n",
    "    - 757/757 [==============================] - 1s 2ms/step - loss: 472.0614\n",
    "    - Epoch 2/6\n",
    "    - 757/757 [==============================] - 1s 2ms/step - loss: 3.4715\n",
    "    - Epoch 3/6\n",
    "    - 757/757 [==============================] - 1s 2ms/step - loss: 0.4602\n",
    "    - Epoch 4/6\n",
    "    - 757/757 [==============================] - 1s 1ms/step - loss: 0.2229\n",
    "    - Epoch 5/6\n",
    "    - 757/757 [==============================] - 1s 1ms/step - loss: 0.1586\n",
    "    - Epoch 6/6\n",
    "    - 757/757 [==============================] - 1s 1ms/step - loss: 0.1518\n",
    "    \n",
    "- trainingdata_add2.pickle, test_size = 0.33, random_state = 321, epoch 6\n",
    "    - Epoch 1/6\n",
    "    - 757/757 [==============================] - 1s 1ms/step - loss: 666.0933\n",
    "    - Epoch 2/6\n",
    "    - 757/757 [==============================] - 1s 1ms/step - loss: 1.5435\n",
    "    - Epoch 3/6\n",
    "    - 757/757 [==============================] - 1s 1ms/step - loss: 0.1681\n",
    "    - Epoch 4/6\n",
    "    - 757/757 [==============================] - 1s 1ms/step - loss: 0.1258A: \n",
    "    - Epoch 5/6\n",
    "    - 757/757 [==============================] - 1s 2ms/step - loss: 0.0964\n",
    "    - Epoch 6/6\n",
    "    - 757/757 [==============================] - 1s 1ms/step - loss: 0.0687"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기록 (괜찮았던 조합)\n",
    "- trainingdata_all.pickle, test_size = 0.33, random_state = 123, epoch = 2 <0.02>\n",
    "    - Epoch 1/2\n",
    "    - 4920/4920 [==============================] - 7s 1ms/step - loss: 38.6281\n",
    "    - Epoch 2/2\n",
    "    - 4920/4920 [==============================] - 7s 1ms/step - loss: 0.1972\n",
    "- trainingdata_all.pickle, test_size = 0.33, random_state = 321, epoch = 2 <0.03>\n",
    "    - Epoch 1/2\n",
    "    - 4920/4920 [==============================] - 7s 1ms/step - loss: 68.4014\n",
    "    - Epoch 2/2\n",
    "    - 4920/4920 [==============================] - 7s 1ms/step - loss: 0.1841\n",
    "- trainingdata_all.pickle, test_size = 0.35, random_state = 321, epoch = 2 <0.02> <비추천?>\n",
    "    - Epoch 1/2\n",
    "    - 4773/4773 [==============================] - 7s 1ms/step - loss: 57.7040\n",
    "    - Epoch 2/2\n",
    "    - 4773/4773 [==============================] - 7s 1ms/step - loss: 0.2754\n",
    "- trainingdata_all.pickle, test_size = 0.33, random_state = 747, epoch = 2 <0.03>\n",
    "    - Epoch 1/2\n",
    "    - 4920/4920 [==============================] - 7s 1ms/step - loss: 84.2031\n",
    "    - Epoch 2/2\n",
    "    - 4920/4920 [==============================] - 7s 1ms/step - loss: 0.1877\n",
    "- trainingdata_add.pickle, test_size = 0.33, random_state = 123, epoch = 6\n",
    "    - Epoch 1/6\n",
    "    - 753/753 [==============================] - 1s 1ms/step - loss: 585.2233\n",
    "    - Epoch 2/6\n",
    "    - 753/753 [==============================] - 1s 1ms/step - loss: 1.2138\n",
    "    - Epoch 3/6\n",
    "    - 753/753 [==============================] - 1s 1ms/step - loss: 0.3951\n",
    "    - Epoch 4/6\n",
    "    - 753/753 [==============================] - 1s 1ms/step - loss: 0.2933\n",
    "    -Epoch 5/6\n",
    "    - 753/753 [==============================] - 1s 1ms/step - loss: 0.1878\n",
    "    - Epoch 6/6\n",
    "    - 753/753 [==============================] - 1s 1ms/step - loss: 0.1411"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
