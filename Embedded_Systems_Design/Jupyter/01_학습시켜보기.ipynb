{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#data = pickle.load( open( \"./template/trainingdata.p\", \"rb\" ), encoding=\"latin1\" )\n",
    "#data = pickle.load(open(\"trainingdata_all.pickle\", \"rb\"), encoding=\"latin1\")\n",
    "data = pickle.load(open(\"trainingdata_add2.pickle\", \"rb\"))\n",
    "n_images = len(data)\n",
    "#test, training = data[0:int(n_images/3)], data[int(n_images/3):]\n",
    "training, test = train_test_split(data, test_size=0.33, random_state=321)\n",
    "\n",
    "def get_training_data():\n",
    "\n",
    "    trX = np.array([np.reshape(a[2],a[2].shape[0]**2) for a in training])\n",
    "    print(np.shape(trX)[1])\n",
    "    trY = np.zeros((len(training)),dtype=np.float)\n",
    "    for i, data in enumerate(training):\n",
    "        trY[i] = float(data[0])\n",
    "    return trX, trY\n",
    "\n",
    "def get_test_data():\n",
    "    teX = np.array([np.reshape(a[2],a[2].shape[0]**2) for a in test])\n",
    "    teY = np.zeros((len(test)),dtype=np.float)\n",
    "    for i, data in enumerate(test):\n",
    "        teY[i] = float(data[0])\n",
    "    return teX,teY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1131"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import tensorflow as tf\n",
    "#import pickle\n",
    "#from get_image_data import *\n",
    "\n",
    "class DNN_Driver():\n",
    "    def __init__(self):\n",
    "        self.trX = None\n",
    "        self.trY = None\n",
    "        self.teX = None\n",
    "        self.teY = None\n",
    "        self.model = None\n",
    "\n",
    "    def tf_learn(self):\n",
    "        self.trX, self.trY = get_training_data()\n",
    "        self.teX, self.teY = get_test_data()\n",
    "\n",
    "        seed = 0\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "        self.model=Sequential()\n",
    "        self.model.add(Dense(512, input_dim=np.shape(self.trX)[1], activation='relu'))\n",
    "        self.model.add(Dense(64, activation='relu'))\n",
    "        self.model.add(Dense(1))\n",
    "\n",
    "        self.model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "        self.model.fit(self.trX, self.trY, epochs=6, batch_size=1)\n",
    "        \n",
    "        Y_prediction = self.model.predict(self.teX).flatten()\n",
    "\n",
    "        for i in range(len(self.teY)):\n",
    "            label = self.teY[i]\n",
    "            pred = Y_prediction[i]\n",
    "            print(\"label:{:.2f}, pred:{:.2f}\".format(label, pred))\n",
    "        return\n",
    "\n",
    "    \n",
    "    def get_direction(img):\n",
    "        print(img.shape)\n",
    "    #    img = np.array([np.reshape(img,img.shape**2)])\n",
    "        ret =  self.model.predict(np.array([img]))\n",
    "        return ret\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    def predict_direction(self, img):\n",
    "        print(img.shape)\n",
    "#        img = np.array([np.reshape(img,img.shape**2)])\n",
    "        ret =  self.model.predict(np.array([img]))\n",
    "        return ret\n",
    "\n",
    "    def get_test_img(self):\n",
    "        img = self.teX[10]\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757 374\n"
     ]
    }
   ],
   "source": [
    "print(len(training), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "Epoch 1/6\n",
      "757/757 [==============================] - 1s 2ms/step - loss: 666.0933\n",
      "Epoch 2/6\n",
      "757/757 [==============================] - 1s 1ms/step - loss: 1.5435\n",
      "Epoch 3/6\n",
      "757/757 [==============================] - 1s 1ms/step - loss: 0.1681\n",
      "Epoch 4/6\n",
      "757/757 [==============================] - 1s 1ms/step - loss: 0.1258\n",
      "Epoch 5/6\n",
      "757/757 [==============================] - 1s 1ms/step - loss: 0.0964\n",
      "Epoch 6/6\n",
      "757/757 [==============================] - 1s 1ms/step - loss: 0.0687\n",
      "label:0.00, pred:0.12\n",
      "label:1.00, pred:1.26\n",
      "label:0.00, pred:-0.18\n",
      "label:0.00, pred:0.31\n",
      "label:-1.00, pred:-0.62\n",
      "label:0.50, pred:1.23\n",
      "label:-0.50, pred:-0.43\n",
      "label:0.50, pred:0.74\n",
      "label:0.00, pred:0.62\n",
      "label:0.00, pred:0.12\n",
      "label:0.00, pred:0.17\n",
      "label:0.00, pred:-0.01\n",
      "label:0.00, pred:0.09\n",
      "label:-0.50, pred:-0.28\n",
      "label:0.00, pred:0.00\n",
      "label:-0.50, pred:-0.58\n",
      "label:0.00, pred:0.02\n",
      "label:0.50, pred:0.68\n",
      "label:-1.00, pred:-0.78\n",
      "label:1.00, pred:1.09\n",
      "label:0.00, pred:0.01\n",
      "label:0.00, pred:-0.03\n",
      "label:0.50, pred:0.00\n",
      "label:0.50, pred:-0.06\n",
      "label:1.00, pred:1.25\n",
      "label:0.00, pred:0.03\n",
      "label:0.00, pred:0.00\n",
      "label:0.00, pred:0.25\n",
      "label:0.00, pred:0.30\n",
      "label:0.00, pred:0.01\n",
      "label:0.50, pred:1.20\n",
      "label:1.00, pred:0.94\n",
      "label:1.00, pred:0.77\n",
      "label:0.00, pred:0.49\n",
      "label:0.50, pred:0.82\n",
      "label:0.00, pred:0.03\n",
      "label:0.00, pred:0.25\n",
      "label:0.00, pred:0.25\n",
      "label:1.00, pred:1.29\n",
      "label:0.00, pred:0.14\n",
      "label:0.00, pred:0.01\n",
      "label:1.00, pred:0.86\n",
      "label:0.00, pred:0.01\n",
      "label:0.00, pred:0.00\n",
      "label:-0.50, pred:-0.20\n",
      "label:1.00, pred:0.48\n",
      "label:0.00, pred:0.17\n",
      "label:1.00, pred:0.25\n",
      "label:0.50, pred:0.24\n",
      "label:0.50, pred:0.81\n",
      "label:0.00, pred:0.75\n",
      "label:0.00, pred:-0.05\n",
      "label:1.00, pred:0.29\n",
      "label:0.50, pred:0.36\n",
      "label:0.00, pred:0.01\n",
      "label:0.50, pred:0.41\n",
      "label:0.00, pred:0.00\n",
      "label:0.00, pred:0.17\n",
      "label:1.00, pred:0.93\n",
      "label:0.00, pred:0.09\n",
      "label:0.00, pred:0.00\n",
      "label:0.00, pred:-0.01\n",
      "label:-1.00, pred:-0.13\n",
      "label:0.00, pred:0.40\n",
      "label:0.00, pred:0.01\n",
      "label:0.50, pred:0.41\n",
      "label:1.00, pred:0.89\n",
      "label:0.00, pred:-0.10\n",
      "label:0.00, pred:0.14\n",
      "label:0.00, pred:0.12\n",
      "label:0.00, pred:0.42\n",
      "label:1.00, pred:0.89\n",
      "label:0.00, pred:0.46\n",
      "label:0.50, pred:0.00\n",
      "label:1.00, pred:0.95\n",
      "label:0.00, pred:0.08\n",
      "label:0.50, pred:0.59\n",
      "label:0.00, pred:0.00\n",
      "label:0.00, pred:-0.03\n",
      "label:0.00, pred:0.08\n",
      "label:0.50, pred:0.20\n",
      "label:0.00, pred:-0.12\n",
      "label:0.00, pred:0.13\n",
      "label:1.00, pred:0.54\n",
      "label:0.00, pred:0.00\n",
      "label:0.50, pred:0.71\n",
      "label:0.00, pred:0.02\n",
      "label:1.00, pred:1.38\n",
      "label:1.00, pred:0.60\n",
      "label:0.50, pred:0.55\n",
      "label:0.50, pred:0.95\n",
      "label:0.50, pred:0.92\n",
      "label:0.00, pred:0.19\n",
      "label:0.00, pred:0.00\n",
      "label:0.00, pred:0.24\n",
      "label:0.00, pred:0.21\n",
      "label:0.00, pred:0.02\n",
      "label:0.00, pred:0.00\n",
      "label:0.00, pred:0.33\n",
      "label:0.50, pred:0.38\n",
      "label:0.00, pred:0.25\n",
      "label:1.00, pred:0.74\n",
      "label:0.50, pred:-0.01\n",
      "label:0.00, pred:0.20\n",
      "label:0.00, pred:0.02\n",
      "label:0.00, pred:0.51\n",
      "label:0.50, pred:1.00\n",
      "label:0.50, pred:0.42\n",
      "label:0.50, pred:0.61\n",
      "label:1.00, pred:0.98\n",
      "label:-1.00, pred:-0.55\n",
      "label:0.50, pred:0.78\n",
      "label:0.00, pred:-0.34\n",
      "label:0.50, pred:-0.06\n",
      "label:0.00, pred:-0.07\n",
      "label:0.00, pred:0.04\n",
      "label:0.00, pred:-0.07\n",
      "label:0.00, pred:0.06\n",
      "label:0.00, pred:0.00\n",
      "label:-1.00, pred:-0.51\n",
      "label:0.00, pred:0.07\n",
      "label:0.50, pred:-0.06\n",
      "label:1.00, pred:0.77\n",
      "label:0.50, pred:0.74\n",
      "label:1.00, pred:0.65\n",
      "label:-1.00, pred:-0.39\n",
      "label:0.00, pred:0.00\n",
      "label:0.50, pred:0.90\n",
      "label:-1.00, pred:-0.58\n",
      "label:-0.50, pred:-0.43\n",
      "label:0.50, pred:0.28\n",
      "label:0.50, pred:0.22\n",
      "label:0.00, pred:-0.02\n",
      "label:0.00, pred:0.08\n",
      "label:0.00, pred:0.00\n",
      "label:0.00, pred:0.40\n",
      "label:0.50, pred:0.31\n",
      "label:0.00, pred:0.17\n",
      "label:1.00, pred:1.03\n",
      "label:0.00, pred:0.25\n",
      "label:0.00, pred:-0.09\n",
      "label:0.00, pred:0.00\n",
      "label:0.00, pred:0.41\n",
      "label:0.00, pred:0.16\n",
      "label:1.00, pred:1.05\n",
      "label:-0.50, pred:2.48\n",
      "label:0.50, pred:0.22\n",
      "label:0.00, pred:0.73\n",
      "label:-1.00, pred:-0.53\n",
      "label:0.00, pred:0.10\n",
      "label:1.00, pred:1.19\n",
      "label:0.00, pred:0.11\n",
      "label:0.50, pred:0.83\n",
      "label:0.00, pred:0.03\n",
      "label:1.00, pred:1.24\n",
      "label:-0.50, pred:-0.30\n",
      "label:0.00, pred:0.36\n",
      "label:0.00, pred:-0.05\n",
      "label:0.00, pred:0.00\n",
      "label:0.00, pred:0.10\n",
      "label:0.50, pred:0.59\n",
      "label:0.00, pred:-0.00\n",
      "label:0.00, pred:0.06\n",
      "label:0.00, pred:0.00\n",
      "label:0.00, pred:0.04\n",
      "label:0.00, pred:0.20\n",
      "label:1.00, pred:1.17\n",
      "label:0.00, pred:0.10\n",
      "label:0.00, pred:0.16\n",
      "label:0.50, pred:0.70\n",
      "label:0.00, pred:0.02\n",
      "label:0.50, pred:0.20\n",
      "label:-0.50, pred:-0.48\n",
      "label:-0.50, pred:-0.40\n",
      "label:0.00, pred:-0.02\n",
      "label:1.00, pred:1.15\n",
      "label:0.00, pred:-0.00\n",
      "label:0.00, pred:0.48\n",
      "label:0.00, pred:0.22\n",
      "label:1.00, pred:0.73\n",
      "label:0.50, pred:0.86\n",
      "label:0.00, pred:0.16\n",
      "label:-0.50, pred:-0.42\n",
      "label:-0.50, pred:0.11\n",
      "label:0.00, pred:0.05\n",
      "label:1.00, pred:0.90\n",
      "label:1.00, pred:0.71\n",
      "label:0.00, pred:0.31\n",
      "label:0.50, pred:0.66\n",
      "label:0.00, pred:0.29\n",
      "label:0.00, pred:-0.01\n",
      "label:-0.50, pred:-1.73\n",
      "label:1.00, pred:1.13\n",
      "label:0.00, pred:0.10\n",
      "label:0.50, pred:0.87\n",
      "label:0.00, pred:-0.03\n",
      "label:1.00, pred:1.12\n",
      "label:1.00, pred:1.18\n",
      "label:0.00, pred:0.00\n",
      "label:0.00, pred:0.08\n",
      "label:0.00, pred:0.00\n",
      "label:0.00, pred:0.32\n",
      "label:0.00, pred:0.27\n",
      "label:0.00, pred:0.06\n",
      "label:0.50, pred:0.78\n",
      "label:0.50, pred:0.17\n",
      "label:0.00, pred:0.60\n",
      "label:1.00, pred:1.17\n",
      "label:1.00, pred:1.08\n",
      "label:0.00, pred:-0.11\n",
      "label:0.50, pred:0.61\n",
      "label:0.00, pred:0.01\n",
      "label:0.50, pred:0.00\n",
      "label:0.00, pred:-0.03\n",
      "label:0.00, pred:0.18\n",
      "label:0.00, pred:0.04\n",
      "label:0.50, pred:0.21\n",
      "label:0.00, pred:0.17\n",
      "label:1.00, pred:1.09\n",
      "label:-1.00, pred:-0.01\n",
      "label:0.00, pred:0.10\n",
      "label:0.50, pred:0.31\n",
      "label:1.00, pred:1.18\n",
      "label:0.50, pred:0.60\n",
      "label:1.00, pred:0.98\n",
      "label:0.00, pred:0.24\n",
      "label:0.00, pred:-3.18\n",
      "label:1.00, pred:0.60\n",
      "label:0.50, pred:0.94\n",
      "label:0.00, pred:0.37\n",
      "label:0.50, pred:0.37\n",
      "label:0.00, pred:-0.14\n",
      "label:1.00, pred:1.03\n",
      "label:0.50, pred:0.18\n",
      "label:0.50, pred:0.71\n",
      "label:0.00, pred:0.19\n",
      "label:1.00, pred:0.50\n",
      "label:0.00, pred:0.33\n",
      "label:-1.00, pred:-0.64\n",
      "label:1.00, pred:0.76\n",
      "label:0.00, pred:0.20\n",
      "label:0.00, pred:-0.17\n",
      "label:1.00, pred:1.15\n",
      "label:0.00, pred:-0.01\n",
      "label:1.00, pred:1.36\n",
      "label:0.00, pred:0.06\n",
      "label:0.00, pred:0.21\n",
      "label:0.00, pred:0.15\n",
      "label:0.50, pred:0.66\n",
      "label:0.00, pred:0.24\n",
      "label:0.00, pred:0.00\n",
      "label:0.00, pred:0.09\n",
      "label:0.50, pred:0.37\n",
      "label:0.00, pred:0.16\n",
      "label:-0.50, pred:-0.14\n",
      "label:0.00, pred:0.02\n",
      "label:0.00, pred:-0.08\n",
      "label:0.50, pred:0.86\n",
      "label:1.00, pred:0.71\n",
      "label:1.00, pred:1.17\n",
      "label:1.00, pred:1.17\n",
      "label:0.50, pred:0.73\n",
      "label:0.50, pred:0.38\n",
      "label:0.50, pred:-0.06\n",
      "label:0.00, pred:0.01\n",
      "label:1.00, pred:0.87\n",
      "label:0.00, pred:0.03\n",
      "label:0.50, pred:0.46\n",
      "label:0.50, pred:0.27\n",
      "label:0.00, pred:0.25\n",
      "label:1.00, pred:1.07\n",
      "label:0.00, pred:0.36\n",
      "label:0.50, pred:-0.02\n",
      "label:0.00, pred:0.04\n",
      "label:0.50, pred:0.00\n",
      "label:0.50, pred:0.11\n",
      "label:0.00, pred:0.10\n",
      "label:0.00, pred:0.58\n",
      "label:0.00, pred:0.30\n",
      "label:-1.00, pred:-0.18\n",
      "label:0.50, pred:0.31\n",
      "label:0.00, pred:-0.02\n",
      "label:0.00, pred:0.03\n",
      "label:1.00, pred:1.09\n",
      "label:1.00, pred:0.53\n",
      "label:1.00, pred:0.73\n",
      "label:0.50, pred:0.12\n",
      "label:-1.00, pred:-0.20\n",
      "label:0.50, pred:0.26\n",
      "label:0.50, pred:0.31\n",
      "label:0.00, pred:0.01\n",
      "label:-0.50, pred:-0.26\n",
      "label:0.00, pred:-0.07\n",
      "label:0.00, pred:0.09\n",
      "label:0.50, pred:0.63\n",
      "label:0.00, pred:0.08\n",
      "label:0.50, pred:0.02\n",
      "label:0.50, pred:0.55\n",
      "label:0.50, pred:0.33\n",
      "label:0.00, pred:0.03\n",
      "label:1.00, pred:1.27\n",
      "label:0.00, pred:0.66\n",
      "label:0.00, pred:-0.00\n",
      "label:0.50, pred:0.08\n",
      "label:0.50, pred:0.68\n",
      "label:0.50, pred:0.73\n",
      "label:0.50, pred:0.33\n",
      "label:1.00, pred:1.13\n",
      "label:0.00, pred:0.10\n",
      "label:0.00, pred:0.06\n",
      "label:0.00, pred:0.00\n",
      "label:1.00, pred:0.69\n",
      "label:1.00, pred:0.69\n",
      "label:1.00, pred:-0.51\n",
      "label:0.50, pred:0.16\n",
      "label:0.00, pred:0.63\n",
      "label:0.00, pred:0.26\n",
      "label:0.50, pred:0.34\n",
      "label:-1.00, pred:-0.23\n",
      "label:1.00, pred:0.74\n",
      "label:0.50, pred:0.18\n",
      "label:0.50, pred:0.14\n",
      "label:0.00, pred:0.25\n",
      "label:0.00, pred:0.00\n",
      "label:0.50, pred:0.44\n",
      "label:1.00, pred:0.92\n",
      "label:0.50, pred:0.53\n",
      "label:0.00, pred:0.27\n",
      "label:0.50, pred:0.50\n",
      "label:0.00, pred:0.49\n",
      "label:1.00, pred:0.71\n",
      "label:0.00, pred:0.07\n",
      "label:0.50, pred:0.44\n",
      "label:0.50, pred:0.62\n",
      "label:1.00, pred:0.48\n",
      "label:0.00, pred:0.00\n",
      "label:0.00, pred:0.27\n",
      "label:0.00, pred:0.18\n",
      "label:0.50, pred:0.88\n",
      "label:0.50, pred:0.65\n",
      "label:0.00, pred:0.60\n",
      "label:1.00, pred:1.16\n",
      "label:0.00, pred:0.28\n",
      "label:0.00, pred:0.12\n",
      "label:0.00, pred:0.34\n",
      "label:0.00, pred:-0.10\n",
      "label:0.50, pred:0.05\n",
      "label:0.00, pred:0.06\n",
      "label:0.50, pred:-0.02\n",
      "label:0.50, pred:0.84\n",
      "label:0.00, pred:0.01\n",
      "label:0.00, pred:0.18\n",
      "label:1.00, pred:0.57\n",
      "label:0.00, pred:0.19\n",
      "label:0.00, pred:0.17\n",
      "label:0.00, pred:0.18\n",
      "label:0.00, pred:0.09\n",
      "label:-0.50, pred:1.36\n",
      "label:0.50, pred:0.70\n",
      "label:1.00, pred:0.53\n",
      "label:0.50, pred:0.79\n",
      "label:0.00, pred:0.02\n",
      "label:0.00, pred:-0.06\n",
      "label:0.00, pred:0.48\n",
      "label:0.00, pred:0.12\n",
      "label:1.00, pred:1.05\n",
      "label:0.50, pred:0.76\n",
      "label:0.50, pred:0.17\n",
      "label:0.50, pred:0.19\n",
      "label:0.00, pred:0.01\n",
      "label:0.00, pred:0.13\n",
      "label:0.50, pred:0.00\n",
      "label:0.00, pred:-0.08\n",
      "label:0.50, pred:0.37\n"
     ]
    }
   ],
   "source": [
    "dnn_driver = DNN_Driver()\n",
    "dnn_driver.tf_learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기록 (add2)\n",
    "- trainingdata_add2.pickle, test_size = 0.33, random_state = 123, epoch 6\n",
    "    - Epoch 1/6\n",
    "    - 757/757 [==============================] - 1s 2ms/step - loss: 472.0614\n",
    "    - Epoch 2/6\n",
    "    - 757/757 [==============================] - 1s 2ms/step - loss: 3.4715\n",
    "    - Epoch 3/6\n",
    "    - 757/757 [==============================] - 1s 2ms/step - loss: 0.4602\n",
    "    - Epoch 4/6\n",
    "    - 757/757 [==============================] - 1s 1ms/step - loss: 0.2229\n",
    "    - Epoch 5/6\n",
    "    - 757/757 [==============================] - 1s 1ms/step - loss: 0.1586\n",
    "    - Epoch 6/6\n",
    "    - 757/757 [==============================] - 1s 1ms/step - loss: 0.1518\n",
    "    \n",
    "- trainingdata_add2.pickle, test_size = 0.33, random_state = 321, epoch 6\n",
    "    - Epoch 1/6\n",
    "    - 757/757 [==============================] - 1s 1ms/step - loss: 666.0933\n",
    "    - Epoch 2/6\n",
    "    - 757/757 [==============================] - 1s 1ms/step - loss: 1.5435\n",
    "    - Epoch 3/6\n",
    "    - 757/757 [==============================] - 1s 1ms/step - loss: 0.1681\n",
    "    - Epoch 4/6\n",
    "    - 757/757 [==============================] - 1s 1ms/step - loss: 0.1258A: \n",
    "    - Epoch 5/6\n",
    "    - 757/757 [==============================] - 1s 2ms/step - loss: 0.0964\n",
    "    - Epoch 6/6\n",
    "    - 757/757 [==============================] - 1s 1ms/step - loss: 0.0687"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기록 (add, 괜찮았던 조합)\n",
    "- trainingdata_all.pickle, test_size = 0.33, random_state = 123, epoch = 2 <0.02>\n",
    "    - Epoch 1/2\n",
    "    - 4920/4920 [==============================] - 7s 1ms/step - loss: 38.6281\n",
    "    - Epoch 2/2\n",
    "    - 4920/4920 [==============================] - 7s 1ms/step - loss: 0.1972\n",
    "- trainingdata_all.pickle, test_size = 0.33, random_state = 321, epoch = 2 <0.03>\n",
    "    - Epoch 1/2\n",
    "    - 4920/4920 [==============================] - 7s 1ms/step - loss: 68.4014\n",
    "    - Epoch 2/2\n",
    "    - 4920/4920 [==============================] - 7s 1ms/step - loss: 0.1841\n",
    "- trainingdata_all.pickle, test_size = 0.35, random_state = 321, epoch = 2 <0.02> <비추천?>\n",
    "    - Epoch 1/2\n",
    "    - 4773/4773 [==============================] - 7s 1ms/step - loss: 57.7040\n",
    "    - Epoch 2/2\n",
    "    - 4773/4773 [==============================] - 7s 1ms/step - loss: 0.2754\n",
    "- trainingdata_all.pickle, test_size = 0.33, random_state = 747, epoch = 2 <0.03>\n",
    "    - Epoch 1/2\n",
    "    - 4920/4920 [==============================] - 7s 1ms/step - loss: 84.2031\n",
    "    - Epoch 2/2\n",
    "    - 4920/4920 [==============================] - 7s 1ms/step - loss: 0.1877\n",
    "- trainingdata_add.pickle, test_size = 0.33, random_state = 123, epoch = 6\n",
    "    - Epoch 1/6\n",
    "    - 753/753 [==============================] - 1s 1ms/step - loss: 585.2233\n",
    "    - Epoch 2/6\n",
    "    - 753/753 [==============================] - 1s 1ms/step - loss: 1.2138\n",
    "    - Epoch 3/6\n",
    "    - 753/753 [==============================] - 1s 1ms/step - loss: 0.3951\n",
    "    - Epoch 4/6\n",
    "    - 753/753 [==============================] - 1s 1ms/step - loss: 0.2933\n",
    "    -Epoch 5/6\n",
    "    - 753/753 [==============================] - 1s 1ms/step - loss: 0.1878\n",
    "    - Epoch 6/6\n",
    "    - 753/753 [==============================] - 1s 1ms/step - loss: 0.1411"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
